<!doctype html><html><head><title>Panning gesture - DemoHut</title><link rel="shortcut icon" href="/demohut/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="/demohut/main.css"><meta charset="UTF-8"></head><body><a class="github-corner" href="https://github.com/ZhujinLi/demohut-src/tree/master/src/subjs/subj-pan" target="_blank" aria-label="View source on GitHub"><svg width="120" height="120" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style></a><div class="jumbotron jumbotron-fluid text-monospace" id="jumbotron"><div class="container" style="position: relative"><h2><a href="/demohut/">DemoHut</a></h2></div></div><div class="container"><form class="text-right" action="/demohut/"><button class="btn btn-outline-primary" type="submit">‚Üê all posts</button></form><div class="display-1" style="font-size: 2rem"><p>Panning gesture</p></div><div id="content"><p>Panning gesture is an important kind of camera interaction in map applications. While we are all accustomed to using it every day, it might not be so easy to implement from a programmer's perspective...</p><div class="sub-title">Problem formulation</div><p>A panning touch gesture starts at a certain screen position \(s_0\) with initial camera state \(c_0\). Each time (\(t\)) the touch position changes to \(s_t=s_0+\Delta_t\), the camera \(c_t\) needs to be updated accordingly.</p><div class="sub-title">I. A naive screen-space approach</div><p>We can simply apply some offset in x- and y- directions in camera space, that is, camera's right- and up- directions in world space, to camera's position:</p><p>$$c_t[pos]=c_0[pos] - \lambda(\Delta_t[x] c_0[right] + \Delta_t[y] c_0[up])$$</p><p>while maintaining the camera's look direction.</p><div style="position:relative;"><div id="gui-1" style="position:absolute;top:10px;left:10px;"></div><canvas id="canvas-1"></canvas></div><p>Try to adjust \(\lambda\) and see what happens. Apparently too small or too large the value results in unnatural experience, and a fixed value would never suit all zoom levels.</p><p>We can make it better by calculating \(\lambda\) precisely.</p><div class="sub-title">II. Calibrated screen-space panning</div><p>We want the world moving consistently with touch movement on the screen, no more, no less. This can be achieved by calibrating \(\lambda\) adaptively:</p><p>$$\lambda=\frac{2 dist(c_0[pos], c_0[target]) tan(\frac{c_0[fovy]}{2})} {viewport[height]}$$</p><p>Despite its complex form, the idea behind is quite simple: making the connection between world-space length and screen-space length. Note that \(target\) could either be the world position of screen center on the ground, or touching position, depending on your need. Here I'll go with the former one since it's simpler.</p><canvas id="canvas-2"></canvas><div class="sub-title">III. World-space panning</div><p>In my experience, screen-space panning is primarily used in CAD applications or debug tools but not games or maps, since it's not friendly for end-users who want to view the scene. Most map apps adopt another control fashion - world-space panning, where the camera moves on a hypothetical plane parallel to the ground during panning:</p><canvas id="canvas-3"></canvas><p>As for the implementation, well, it's not something brand new. On the foundation of the previous method, all we need to do is project the y- movement on the plane, by replacing the camera's up by the product of ground's up and camera's right.</p><p>Also note that a compensation term \(u\) regarding the camera tilting is usually needed, or the panning would be too slow with large tilt angles due to perspective foreshortening.</p><p>$$u=\frac{1}{cos(c_0[tilt])}$$</p><p>And the solution turns out to be:</p><p>$$c_t[pos]=c_0[pos] + \lambda(\Delta_t[x] c_0[right] + u\Delta_t[y] ((0, 0, 1) \times c_0[right]))$$</p><div class="sub-title">IV. Tracing world-space panning</div><p>If our goal is to make the starting world position always stay with the touch, then we need some extra work, for the above algorithm assumes a fixed \(\lambda\) during panning where it should actually be be variant: the further the touching position is, the larger.</p><p>We can take a different approach that solves this problem elegantly. Initially the world coordinate at touching position \(s_0\) is \(w_0\). At the moment \(t\) the touch moves to a new position, while the camera remains its last state. As a result, the touching world position becomes \(w_t\) but not \(w_0\). To correct it, we have to translate the camera in the opposite direction:</p><p>$$c_t[pos, target]=c_t[pos, target]+(w_0-w_t)$$</p><p>There remains only one problem to solve: how to calculate the world position corresponding to a screen coordinate? Note the world position is always on the ground, the problem can be transformed to solving of the intersection between a ray and a plane with \(z=0\), which has pretty efficient solution in real-time.</p><canvas id="canvas-4"></canvas><p>This method has its own downsides though. For one, the world position must be computable, so map applications that render the world as an virtual globe would have problems when the touch lies outside the globe. Another problem is that while the world position follows the touch faithfully, it can be moving too fast and annoys users if the touch is near the horizon. So map apps generally prefer the non-tracing approach in practice.</p><div class="sub-title">V. 3D tracing world-space panning</div><p>The best place to apply tracing approach is scenes with 3D terrain. If the touch points to the side of a mountain, it is generally expected that position stays with the touch.</p><p>The major challenge of it, compared to the 2D version, is that the touching position is not on the ground. Yet there's a simple workaround: after the initial position \(w_0\) is determined (through ray-object intersection solving), we can treat the plane \(z=w_0[z]\) as the ground, and perform subsequence operations as before.</p><canvas id="canvas-5"></canvas><p><i>3D Model courtesy of Google from <a href="https://poly.google.com/view/3NvfPMBZrBQ">Poly</a></i></p><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script><script src="./subj-pan.js"></script></div><div class="hr-theme-slash-2"><div class="hr-line l"></div><div class="hr-icon"><a href="https://ZhujinLi.github.io"><img src="/demohut/res/home.png" height="28px"></a></div><div class="hr-line r"></div></div><p class="text-center text-secondary" style="font-size: 15px">All demos have been tested on Microsoft Edge/macOS.</p></div><script src="/demohut/main.js"></script></body></html>